{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7xufxk66W01l5p5OdIY96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahajbane/notebooks/blob/main/LLMs_0503.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huggingface Model Config for fine-tunig"
      ],
      "metadata": {
        "id": "sACbzh_buXBx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHYqBgntrtHY"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCasualLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "base_model = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "\n",
        "try:\n",
        "  model = AutoModelForCasualLM.from_pretrained(\n",
        "      base_model,\n",
        "      device_map = {\"\" : 0} if torch.cuda.is_available() else {\"\" : \"cpu\"}\n",
        "  )\n",
        "\n",
        "except RuntimeError:\n",
        "  print(\"Switching to CPU due to memory issues.\")\n",
        "  model = AutoModelForCasualLM.from_pretrained(base_model, device_map = {\"\" : \"cpu\"})\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, device_map = {\"\":\"cpu\"})\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "uly2Vso_uu_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, logging\n",
        "\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "prompt = \"What do you thinkg about the movie Inception?\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    task = \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    max_length = 50\n",
        ")\n",
        "\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text']) # generated text needs to be specified"
      ],
      "metadata": {
        "id": "jQlu0xWHvTXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_params = LoraConfig( #cont\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "OgJsvS-NvE6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JfdkdVNYvv0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
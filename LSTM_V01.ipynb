{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahajbane/notebooks/blob/main/LSTM_V01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o0xaFSUY-miD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Time Series Forecasting Tutorial: Apple Stock Price Prediction\n",
        "\n",
        "This notebook provides a comprehensive tutorial on using Long Short-Term Memory (LSTM) networks for time series forecasting. We'll predict Apple stock prices using historical data, implementing best practices to prevent overfitting and ensure model performance.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction to LSTM for Time Series](#introduction)\n",
        "2. [Environment Setup](#setup)\n",
        "3. [Data Preparation and Exploration](#data-preparation)\n",
        "4. [Creating the LSTM Model Architecture](#lstm-architecture)\n",
        "5. [Training and Validation Process](#training-process)\n",
        "6. [Evaluation and Visualization](#evaluation)\n",
        "7. [Model Improvements and Best Practices](#improvements)\n",
        "8. [Conclusion](#conclusion)\n",
        "\n",
        "<a id=\"introduction\"></a>\n",
        "## 1. Introduction to LSTM for Time Series\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) architecture designed to work with sequential data. Unlike traditional neural networks, LSTMs have internal mechanisms called gates that can regulate the flow of information, allowing them to capture long-term dependencies in time series data.\n",
        "\n",
        "### Why LSTM for Stock Prediction?\n",
        "\n",
        "- **Memory capabilities**: LSTMs can \"remember\" patterns over long sequences\n",
        "- **Resistance to vanishing/exploding gradients**: A common problem in traditional RNNs\n",
        "- **Ability to capture non-linear relationships**: Important for financial time series\n",
        "- **Flexible architecture**: Can be configured for different prediction tasks\n",
        "\n",
        "### LSTM Gates Explained\n",
        "\n",
        "LSTMs contain three main gates that regulate information flow:\n",
        "\n",
        "1. **Forget Gate**: Controls what information from the previous cell state should be discarded\n",
        "   - Formula: f_t = σ(W_f · [h_{t-1}, x_t] + b_f)\n",
        "\n",
        "2. **Input Gate**: Determines what new information should be stored in the cell state\n",
        "   - Input gate layer: i_t = σ(W_i · [h_{t-1}, x_t] + b_i)\n",
        "   - Candidate values: C̃_t = tanh(W_C · [h_{t-1}, x_t] + b_C)\n",
        "\n",
        "3. **Output Gate**: Controls what information from the cell state goes to the output\n",
        "   - Formula: o_t = σ(W_o · [h_{t-1}, x_t] + b_o)\n",
        "\n",
        "The cell state update combines these gates:\n",
        "- C_t = f_t * C_{t-1} + i_t * C̃_t\n",
        "- h_t = o_t * tanh(C_t)\n",
        "\n",
        "<a id=\"setup\"></a>\n",
        "## 2. Environment Setup\n",
        "\n",
        "Let's import the necessary libraries for our implementation:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from copy import deepcopy as dc\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set device for computation\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set the seed\n",
        "set_seed(42)\n",
        "```\n",
        "\n",
        "### Importance of Setting Seeds\n",
        "\n",
        "Setting random seeds ensures that your model training is reproducible. This is crucial for:\n",
        "- Debugging and troubleshooting\n",
        "- Sharing results with others\n",
        "- Comparing different model configurations fairly\n",
        "\n",
        "<a id=\"data-preparation\"></a>\n",
        "## 3. Data Preparation and Exploration\n",
        "\n",
        "Proper data preparation is critical for time series forecasting. We need to:\n",
        "1. Load and visualize the time series data\n",
        "2. Create lag features (using previous time steps to predict the next)\n",
        "3. Normalize the data to a suitable range\n",
        "4. Split into training and testing sets\n",
        "5. Reshape for LSTM input requirements\n",
        "\n",
        "```python\n",
        "# Load and prepare the data\n",
        "def load_and_prepare_data(n_steps=5, train_size=0.8):\n",
        "    \"\"\"\n",
        "    Load and prepare time series data for LSTM model.\n",
        "    \n",
        "    Args:\n",
        "        n_steps (int): Number of lag steps to use as features\n",
        "        train_size (float): Proportion of data to use for training\n",
        "        \n",
        "    Returns:\n",
        "        tuple: Prepared data and preprocessing objects\n",
        "    \"\"\"\n",
        "    # Load and process Apple stock data\n",
        "    timec = TimeConfig()\n",
        "    df = timec.process_chain(path='../Data/apple.csv', columns=['Close/Last'])\n",
        "    df = df.loc['2021':'2024'].copy()\n",
        "    \n",
        "    # Visualize the time series data\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    df[\"Value\"].plot(label=\"Apple Stock\", color=\"orange\")\n",
        "    plt.title(\"Apple Stock Analysis\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.gca().set_facecolor(\"#E8E8E4\")\n",
        "    plt.gcf().set_facecolor(\"#E8E8E4\")\n",
        "    plt.legend(facecolor=\"#E8E8E4\")\n",
        "    plt.savefig('apple_stock_data.png')  # Save the plot\n",
        "    plt.close()\n",
        "    \n",
        "    # Create lag features\n",
        "    timef = TransformLag()\n",
        "    data = timef.lag_transform(df, n_steps)\n",
        "    \n",
        "    # Convert to numpy array\n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    # Normalize the data to [-1, 1] range\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    transformed = scaler.fit_transform(data)\n",
        "    \n",
        "    # Split into features (X) and target (y)\n",
        "    X = transformed[:, 1:]\n",
        "    y = transformed[:, 0]\n",
        "    \n",
        "    # Flip the order of lag features (for time series causality)\n",
        "    X = dc(np.flip(X, axis=1))\n",
        "    \n",
        "    # Split into training and testing sets\n",
        "    split_index = int(len(X) * train_size)\n",
        "    X_train = X[:split_index]\n",
        "    X_test = X[split_index:]\n",
        "    y_train = y[:split_index]\n",
        "    y_test = y[split_index:]\n",
        "    \n",
        "    # Reshape to match LSTM's expected input: [samples, sequence_length, features]\n",
        "    X_train = X_train.reshape((-1, n_steps, 1))\n",
        "    X_test = X_test.reshape((-1, n_steps, 1))\n",
        "    y_train = y_train.reshape((-1, 1))\n",
        "    y_test = y_test.reshape((-1, 1))\n",
        "    \n",
        "    # Convert to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train).float()\n",
        "    X_test = torch.tensor(X_test).float()\n",
        "    y_train = torch.tensor(y_train).float()\n",
        "    y_test = torch.tensor(y_test).float()\n",
        "    \n",
        "    return (X_train, y_train, X_test, y_test, scaler, df)\n",
        "```\n",
        "\n",
        "### Understanding Time Series Feature Engineering\n",
        "\n",
        "- **Lag Features**: Using previous time steps (t-1, t-2, ..., t-n) to predict the current value (t)\n",
        "- **Number of Steps (n_steps)**: The lookback window or context window for prediction\n",
        "- **Data Normalization**: Crucial for neural networks, as it:\n",
        "  - Accelerates convergence\n",
        "  - Prevents features with larger scales from dominating\n",
        "  - Places all features on the same scale (-1 to 1 in our case)\n",
        "\n",
        "### Custom Dataset for PyTorch\n",
        "\n",
        "To efficiently feed data to our LSTM model, we create a custom PyTorch Dataset:\n",
        "\n",
        "```python\n",
        "# Custom Dataset class for time series\n",
        "class TimeSeries(Dataset):\n",
        "    \"\"\"Custom PyTorch Dataset for time series data.\"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]\n",
        "```\n",
        "\n",
        "<a id=\"lstm-architecture\"></a>\n",
        "## 4. Creating the LSTM Model Architecture\n",
        "\n",
        "Now we'll define our LSTM model. The architecture consists of:\n",
        "1. One or more LSTM layers\n",
        "2. Dropout for regularization\n",
        "3. A fully connected (Linear) output layer\n",
        "\n",
        "```python\n",
        "# Define the LSTM model\n",
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long Short-Term Memory (LSTM) network for time series forecasting.\n",
        "    \n",
        "    Implements a stacked LSTM with dropout for regularization and\n",
        "    a fully connected layer as the output layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_stacked_layers, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        Initialize the LSTM model.\n",
        "        \n",
        "        Args:\n",
        "            input_size (int): Number of input features (typically 1 for univariate time series)\n",
        "            hidden_size (int): Number of features in the hidden state\n",
        "            num_stacked_layers (int): Number of stacked LSTM layers\n",
        "            dropout_rate (float): Dropout rate for regularization\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_stacked_layers = num_stacked_layers\n",
        "        \n",
        "        # LSTM layer with dropout\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers,\n",
        "                           batch_first=True, dropout=dropout_rate)\n",
        "        \n",
        "        # Output fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the LSTM.\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape [batch_size, sequence_length, features]\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Output predictions\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Initialize hidden state and cell state with zeros\n",
        "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        \n",
        "        # Pass input through LSTM layers\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        \n",
        "        # Use the final time step's output for prediction\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "```\n",
        "\n",
        "### Key Components of the LSTM Model\n",
        "\n",
        "- **input_size**: The number of features per time step (1 for univariate time series)\n",
        "- **hidden_size**: The number of features in the hidden state (higher = more capacity)\n",
        "- **num_stacked_layers**: Number of LSTM layers stacked on top of each other\n",
        "- **dropout_rate**: Rate at which neurons are randomly \"dropped\" during training (regularization)\n",
        "- **batch_first=True**: Input shape is [batch_size, sequence_length, features]\n",
        "- **h0 and c0**: Initial hidden and cell states (initialized to zeros)\n",
        "\n",
        "<a id=\"training-process\"></a>\n",
        "## 5. Training and Validation Process\n",
        "\n",
        "Training an LSTM model requires:\n",
        "1. A training function for each epoch\n",
        "2. A validation function to monitor performance\n",
        "3. Mechanisms to prevent overfitting (early stopping, learning rate scheduling)\n",
        "\n",
        "```python\n",
        "# Training function\n",
        "def train_one_epoch(model, train_loader, optimizer, loss_function, device):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): The neural network model\n",
        "        train_loader (DataLoader): DataLoader for training data\n",
        "        optimizer (Optimizer): Optimizer for updating model weights\n",
        "        loss_function (Loss): Loss function for training\n",
        "        device (str): Device to perform computations on ('cpu' or 'cuda')\n",
        "        \n",
        "    Returns:\n",
        "        float: Average training loss for the epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "        batch_size = x_batch.size(0)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        output = model(x_batch)\n",
        "        loss = loss_function(output, y_batch)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Accumulate loss\n",
        "        running_loss += loss.item() * batch_size\n",
        "        total_samples += batch_size\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_train_loss = running_loss / total_samples\n",
        "    return avg_train_loss\n",
        "\n",
        "# Validation function\n",
        "def validate(model, val_loader, loss_function, device):\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): The neural network model\n",
        "        val_loader (DataLoader): DataLoader for validation data\n",
        "        loss_function (Loss): Loss function for evaluation\n",
        "        device (str): Device to perform computations on ('cpu' or 'cuda')\n",
        "        \n",
        "    Returns:\n",
        "        tuple: Average validation loss and model predictions\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    total_samples = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for batch in val_loader:\n",
        "            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "            batch_size = x_batch.size(0)\n",
        "            \n",
        "            # Forward pass\n",
        "            output = model(x_batch)\n",
        "            loss = loss_function(output, y_batch)\n",
        "            \n",
        "            # Accumulate loss\n",
        "            running_loss += loss.item() * batch_size\n",
        "            total_samples += batch_size\n",
        "            \n",
        "            # Store predictions and targets for metrics calculation\n",
        "            all_predictions.append(output.cpu().numpy())\n",
        "            all_targets.append(y_batch.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_predictions = np.concatenate(all_predictions)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "    \n",
        "    # Calculate average loss\n",
        "    avg_val_loss = running_loss / total_samples\n",
        "    return avg_val_loss, all_predictions, all_targets\n",
        "```\n",
        "\n",
        "### Understanding torch.no_grad()\n",
        "\n",
        "The `with torch.no_grad():` context manager is crucial during validation:\n",
        "\n",
        "- **Purpose**: Temporarily disables gradient calculation\n",
        "- **Benefits**:\n",
        "  - Reduces memory consumption (no gradients stored)\n",
        "  - Speeds up computation (no backward pass preparation)\n",
        "  - Prevents accidental parameter updates during evaluation\n",
        "- **When to use**: Always in validation and inference, never in training\n",
        "\n",
        "### Complete Training Loop with Early Stopping\n",
        "\n",
        "Now we implement the full training loop with early stopping and learning rate scheduling:\n",
        "\n",
        "```python\n",
        "# Main training function with early stopping\n",
        "def train_model(model, X_train, y_train, X_val, y_val, batch_size=32,\n",
        "                learning_rate=0.001, max_epochs=100, patience=10,\n",
        "                min_delta=0.001, enable_scheduler=True):\n",
        "    \"\"\"\n",
        "    Train the LSTM model with early stopping and learning rate scheduling.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): The neural network model\n",
        "        X_train (torch.Tensor): Training features\n",
        "        y_train (torch.Tensor): Training targets\n",
        "        X_val (torch.Tensor): Validation features\n",
        "        y_val (torch.Tensor): Validation targets\n",
        "        batch_size (int): Batch size for training\n",
        "        learning_rate (float): Initial learning rate\n",
        "        max_epochs (int): Maximum number of epochs to train\n",
        "        patience (int): Number of epochs to wait for improvement before stopping\n",
        "        min_delta (float): Minimum change in validation loss to qualify as improvement\n",
        "        enable_scheduler (bool): Whether to use learning rate scheduling\n",
        "        \n",
        "    Returns:\n",
        "        tuple: Trained model, training history, best validation loss\n",
        "    \"\"\"\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TimeSeries(X_train, y_train)\n",
        "    val_dataset = TimeSeries(X_val, y_val)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    # Define loss function and optimizer\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    \n",
        "    # Learning rate scheduler\n",
        "    scheduler = None\n",
        "    if enable_scheduler:\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=patience // 3,\n",
        "            verbose=True, min_lr=1e-6\n",
        "        )\n",
        "    \n",
        "    # Initialize variables for early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(max_epochs):\n",
        "        # Train one epoch\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_function, device)\n",
        "        \n",
        "        # Validate\n",
        "        val_loss, _, _ = validate(model, val_loader, loss_function, device)\n",
        "        \n",
        "        # Store losses\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{max_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
        "        \n",
        "        # Update learning rate if scheduler is enabled\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_loss)\n",
        "        \n",
        "        # Check for improvement\n",
        "        if val_loss < best_val_loss - min_delta:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = dc(model.state_dict())\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "        \n",
        "        # Early stopping\n",
        "        if counter >= patience:\n",
        "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
        "            break\n",
        "    \n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        \n",
        "    return model, {'train_losses': train_losses, 'val_losses': val_losses}, best_val_loss\n",
        "```\n",
        "\n",
        "### Anti-Overfitting Techniques\n",
        "\n",
        "This implementation includes several techniques to prevent overfitting:\n",
        "\n",
        "1. **Early Stopping**: Stops training when validation loss stops improving\n",
        "   - **patience**: Number of epochs to wait for improvement\n",
        "   - **min_delta**: Minimum improvement threshold\n",
        "\n",
        "2. **Learning Rate Scheduling**: Reduces learning rate when progress plateaus\n",
        "   - Uses ReduceLROnPlateau scheduler\n",
        "   - Helps fine-tune model weights when nearing convergence\n",
        "\n",
        "3. **Weight Decay**: L2 regularization penalty on model weights\n",
        "   - Implemented through the optimizer (weight_decay=1e-5)\n",
        "   - Discourages large weight values\n",
        "\n",
        "4. **Dropout**: Randomly ignores neurons during training\n",
        "   - Applied in the LSTM model definition\n",
        "   - Forces the network to learn redundant representations\n",
        "\n",
        "<a id=\"evaluation\"></a>\n",
        "## 6. Evaluation and Visualization\n",
        "\n",
        "After training, we need to evaluate model performance and visualize results:\n",
        "\n",
        "```python\n",
        "# Inverse transform predictions back to original scale\n",
        "def inverse_transform_predictions(predictions, targets, scaler):\n",
        "    \"\"\"\n",
        "    Transform normalized predictions and targets back to original scale.\n",
        "    \n",
        "    Args:\n",
        "        predictions (numpy.ndarray): Model predictions in normalized scale\n",
        "        targets (numpy.ndarray): Actual targets in normalized scale\n",
        "        scaler (MinMaxScaler): Scaler used for normalization\n",
        "        \n",
        "    Returns:\n",
        "        tuple: Predictions and targets in original scale\n",
        "    \"\"\"\n",
        "    # Flatten if needed\n",
        "    pred_flat = predictions.flatten()\n",
        "    target_flat = targets.flatten()\n",
        "    \n",
        "    # Create dummy arrays for inverse transformation\n",
        "    pred_dummy = np.zeros((len(pred_flat), 2))\n",
        "    pred_dummy[:, 0] = pred_flat\n",
        "    \n",
        "    target_dummy = np.zeros((len(target_flat), 2))\n",
        "    target_dummy[:, 0] = target_flat\n",
        "    \n",
        "    # Inverse transform\n",
        "    pred_inverted = scaler.inverse_transform(pred_dummy)[:, 0]\n",
        "    target_inverted = scaler.inverse_transform(target_dummy)[:, 0]\n",
        "    \n",
        "    return pred_inverted, target_inverted\n",
        "\n",
        "# Calculate performance metrics\n",
        "def calculate_metrics(predictions, targets):\n",
        "    \"\"\"\n",
        "    Calculate regression performance metrics.\n",
        "    \n",
        "    Args:\n",
        "        predictions (numpy.ndarray): Model predictions\n",
        "        targets (numpy.ndarray): Actual targets\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionary of performance metrics\n",
        "    \"\"\"\n",
        "    mse = mean_squared_error(targets, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(targets, predictions)\n",
        "    r2 = r2_score(targets, predictions)\n",
        "    \n",
        "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
        "    mape = np.mean(np.abs((targets - predictions) / np.abs(targets + 1e-10))) * 100\n",
        "    \n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'MAPE': mape\n",
        "    }\n",
        "\n",
        "# Plotting function for actual vs predicted values\n",
        "def plot_predictions(predictions, actual, title, filename):\n",
        "    \"\"\"\n",
        "    Plot actual vs. predicted values.\n",
        "    \n",
        "    Args:\n",
        "        predictions (numpy.ndarray): Model predictions\n",
        "        actual (numpy.ndarray): Actual values\n",
        "        title (str): Plot title\n",
        "        filename (str): File to save the plot to\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(actual, label='Actual', color='blue')\n",
        "    plt.plot(predictions, label='Predicted', color='red')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Stock Price')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "# Plot learning curves\n",
        "def plot_learning_curves(train_losses, val_losses, filename):\n",
        "    \"\"\"\n",
        "    Plot training and validation loss curves.\n",
        "    \n",
        "    Args:\n",
        "        train_losses (list): Training losses per epoch\n",
        "        val_losses (list): Validation losses per epoch\n",
        "        filename (str): File to save the plot to\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "```\n",
        "\n",
        "### Key Evaluation Metrics for Time Series\n",
        "\n",
        "For time series forecasting, these metrics provide valuable insights:\n",
        "\n",
        "1. **MSE (Mean Squared Error)**: Average of squared differences between predictions and actual values\n",
        "2. **RMSE (Root Mean Squared Error)**: Square root of MSE, in the same unit as the original data\n",
        "3. **MAE (Mean Absolute Error)**: Average of absolute differences, less sensitive to outliers than MSE\n",
        "4. **R² (R-squared)**: Proportion of variance explained by the model (higher is better)\n",
        "5. **MAPE (Mean Absolute Percentage Error)**: Average percentage error, scale-independent\n",
        "\n",
        "### Visualizing Results\n",
        "\n",
        "Visual analysis helps understand model performance:\n",
        "\n",
        "1. **Learning Curves**: Plot training vs. validation loss to detect overfitting\n",
        "2. **Predictions vs. Actual**: Compare predicted values with actual values\n",
        "3. **Residual Analysis**: Examine the difference between predicted and actual values\n",
        "\n",
        "<a id=\"improvements\"></a>\n",
        "## 7. Model Improvements and Best Practices\n",
        "\n",
        "Here's the main execution function that brings everything together:\n",
        "\n",
        "```python\n",
        "# Main execution\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    # Configuration\n",
        "    n_steps = 5  # Increased from 1 to 5 for more context\n",
        "    batch_size = 32\n",
        "    hidden_size = 50  # Increased from 4\n",
        "    num_stacked_layers = 2  # Increased from 1\n",
        "    dropout_rate = 0.3  # Increased from 0.05\n",
        "    learning_rate = 0.001\n",
        "    max_epochs = 100\n",
        "    patience = 10\n",
        "    \n",
        "    # Create output directory\n",
        "    os.makedirs('outputs', exist_ok=True)\n",
        "    \n",
        "    # Load and prepare data\n",
        "    X_train, y_train, X_test, y_test, scaler, df = load_and_prepare_data(n_steps=n_steps)\n",
        "    \n",
        "    # Print dataset information\n",
        "    print(f\"Dataset Info:\")\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "    print(f\"y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}\")\n",
        "    print(f\"y_test shape: {y_test.shape}\")\n",
        "    \n",
        "    # Initialize model\n",
        "    model = LSTM(\n",
        "        input_size=1,\n",
        "        hidden_size=hidden_size,\n",
        "        num_stacked_layers=num_stacked_layers,\n",
        "        dropout_rate=dropout_rate\n",
        "    ).to(device)\n",
        "    \n",
        "    # Print model architecture\n",
        "    print(f\"Model Architecture:\")\n",
        "    print(model)\n",
        "    \n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    trained_model, history, best_val_loss = train_model(\n",
        "        model, X_train, y_train, X_test, y_test,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        max_epochs=max_epochs,\n",
        "        patience=patience\n",
        "    )\n",
        "    \n",
        "    # Plot learning curves\n",
        "    plot_learning_curves(\n",
        "        history['train_losses'],\n",
        "        history['val_losses'],\n",
        "        'outputs/learning_curves.png'\n",
        "    )\n",
        "    \n",
        "    # Evaluate model\n",
        "    print(\"Evaluating model...\")\n",
        "    # Create DataLoaders for evaluation\n",
        "    train_dataset = TimeSeries(X_train, y_train)\n",
        "    test_dataset = TimeSeries(X_test, y_test)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    # Loss function\n",
        "    loss_function = nn.MSELoss()\n",
        "    \n",
        "    # Evaluate on training set\n",
        "    train_loss, train_pred, train_targets = validate(trained_model, train_loader, loss_function, device)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    test_loss, test_pred, test_targets = validate(trained_model, test_loader, loss_function, device)\n",
        "    \n",
        "    # Transform predictions back to original scale\n",
        "    train_pred_orig, train_targets_orig = inverse_transform_predictions(\n",
        "        train_pred, train_targets, scaler)\n",
        "    test_pred_orig, test_targets_orig = inverse_transform_predictions(\n",
        "        test_pred, test_targets, scaler)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_metrics = calculate_metrics(train_pred_orig, train_targets_orig)\n",
        "    test_metrics = calculate_metrics(test_pred_orig, test_targets_orig)\n",
        "    \n",
        "    # Plot predictions\n",
        "    plot_predictions(train_pred_orig, train_targets_orig,\n",
        "                    'Training: Actual vs Predicted', 'outputs/train_predictions.png')\n",
        "    plot_predictions(test_pred_orig, test_targets_orig,\n",
        "                    'Test: Actual vs Predicted', 'outputs/test_predictions.png')\n",
        "    \n",
        "    # Print evaluation metrics\n",
        "    print(\"\\nTraining Metrics:\")\n",
        "    for metric, value in train_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    \n",
        "    print(\"\\nTest Metrics:\")\n",
        "    for metric, value in test_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    \n",
        "    # Save trained model\n",
        "    torch.save(trained_model.state_dict(), 'outputs/lstm_model.pth')\n",
        "    print(\"Model saved to outputs/lstm_model.pth\")\n",
        "    \n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```\n",
        "\n",
        "### Best Practices for LSTM Time Series Models\n",
        "\n",
        "1. **Increase Context Window**: Using n_steps=5 instead of just 1 provides more historical context\n",
        "2. **Proper Model Sizing**:\n",
        "   - Increased hidden_size from 4 to 50\n",
        "   - Increased num_stacked_layers from 1 to 2\n",
        "3. **Strong Regularization**:\n",
        "   - Increased dropout from 0.05 to 0.3\n",
        "   - Added weight decay in optimizer\n",
        "4. **Smart Training Process**:\n",
        "   - Early stopping with patience=10\n",
        "   - Learning rate scheduling\n",
        "   - Proper batch size (32)\n",
        "5. **Comprehensive Evaluation**:\n",
        "   - Multiple metrics (RMSE, MAE, R², MAPE)\n",
        "   - Visualization tools\n",
        "\n",
        "<a id=\"conclusion\"></a>\n",
        "## 8. Conclusion\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **LSTM Architecture**: Powerful for time series due to its memory capabilities and gate mechanisms\n",
        "2. **Data Preparation**: Critical steps include normalization, lag features, and proper reshaping\n",
        "3. **Overfitting Prevention**: Multiple techniques including dropout, early stopping, and L2 regularization\n",
        "4. **Evaluation**: Always evaluate on both training and test sets with multiple metrics\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "| Issue | Solution |\n",
        "|-------|----------|\n",
        "| Overfitting | Increase dropout, add weight decay, implement early stopping |\n",
        "| Underfitting | Increase model capacity (hidden size, layers), add more features |\n",
        "| Vanishing Gradients | Use LSTM instead of basic RNN, ensure proper weight initialization |\n",
        "| Training Instability | Reduce learning rate, clip gradients, normalize inputs |\n"
      ],
      "metadata": {
        "id": "LIvaMeZ_-ntS"
      }
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahajbane/notebooks/blob/main/PlasticDetection_S2_Annotations_Analysis_mujtaba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUNCKl8XfSC5"
      },
      "source": [
        "# Plastic Detection from Sentinel-2 Data – Phase 1: Annotation Evaluation\n",
        "\n",
        "This notebook initiates the pipeline for training a plastic litter segmentation model using the **Mediterranean Sentinel-2 Litter Windrows Catalogue (2015–2021)**.\n",
        "\n",
        "### ✅ Objectives of Phase 1:\n",
        "- Load and explore the `.nc` dataset containing 14,374 plastic filament detections.\n",
        "- Verify centroid and pixel-level annotations.\n",
        "- Confirm suitability of annotations for generating ground truth segmentation masks.\n",
        "- Visualize sample binary masks based on annotated pixels.\n",
        "\n",
        "### ✅ Outcome:\n",
        "We successfully loaded the dataset, extracted filament coordinates, and created a binary mask from the first sample, confirming that the dataset is suitable for training segmentation models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZTSyXmfI9wh",
        "outputId": "43c6b5b6-7ad2-45a8-f59e-a97635c76c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netCDF4\n",
            "  Downloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2025.1.31)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2.0.2)\n",
            "Downloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.4.post1 netCDF4-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install netCDF4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "JCSZL-K7J7GQ",
        "outputId": "c8662c1c-8d65-4357-b180-3c189502e2bd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1eb8a41f00d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetCDF4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from netCDF4 import Dataset\n",
        "\n",
        "file_path = '/content/drive/MyDrive/WASP_LW_SENT2_MED_L1C_B_201506_202109_10m_6y_NRT_v1.0.nc'\n",
        "#file_path = 'WASP_LW_SENT2_MED_L1C_B_201506_202109_10m_6y_NRT_v1.0.nc'\n",
        "ds = Dataset(file_path, mode='r')\n",
        "\n",
        "print(\"Available variables:\")\n",
        "print(ds.variables.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItjNIXbEdq0I"
      },
      "outputs": [],
      "source": [
        "# Step 1: Explore centroid information for the first 5 filaments\n",
        "print(\"Latitude Centroids:\", ds.variables['lat_centroid'][:5])\n",
        "print(\"Longitude Centroids:\", ds.variables['lon_centroid'][:5])\n",
        "print(\"X Centroids:\", ds.variables['x_centroid'][:5])\n",
        "print(\"Y Centroids:\", ds.variables['y_centroid'][:5])\n",
        "\n",
        "# Step 2: Check number of pixels in each filament (first 5)\n",
        "print(\"Number of pixels in filament:\", ds.variables['n_pixels_fil'][:5])\n",
        "\n",
        "# Step 3: Explore pixel locations for the first filament\n",
        "pixels_x = ds.variables['pixel_x'][0]  # shape: [n_max_pixels_fil]\n",
        "pixels_y = ds.variables['pixel_y'][0]\n",
        "\n",
        "# Filter out padding values (-999)\n",
        "valid_pixel_indices = pixels_x != -999\n",
        "pixels_x = pixels_x[valid_pixel_indices]\n",
        "pixels_y = pixels_y[valid_pixel_indices]\n",
        "\n",
        "# Print first 10 valid pixel coordinates\n",
        "print(\"Sample pixel (x, y) coordinates:\")\n",
        "for x, y in zip(pixels_x[:10], pixels_y[:10]):\n",
        "    print(f\"({x}, {y})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKh_gOyqdrfF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define size of the mask canvas (adjustable – here, centered around filament)\n",
        "canvas_size = 300  # pixels\n",
        "center_x = ds.variables['x_centroid'][0]\n",
        "center_y = ds.variables['y_centroid'][0]\n",
        "\n",
        "# Create empty binary mask\n",
        "mask = np.zeros((canvas_size, canvas_size), dtype=np.uint8)\n",
        "\n",
        "# Get valid pixel coordinates for the first filament\n",
        "pixels_x = ds.variables['pixel_x'][0]\n",
        "pixels_y = ds.variables['pixel_y'][0]\n",
        "valid = pixels_x != -999\n",
        "pixels_x = pixels_x[valid]\n",
        "pixels_y = pixels_y[valid]\n",
        "\n",
        "# Offset coordinates to center mask on filament\n",
        "for x, y in zip(pixels_x, pixels_y):\n",
        "    rel_x = x - center_x + canvas_size // 2\n",
        "    rel_y = y - center_y + canvas_size // 2\n",
        "    if 0 <= rel_x < canvas_size and 0 <= rel_y < canvas_size:\n",
        "        mask[rel_y, rel_x] = 1  # Set plastic presence = 1\n",
        "\n",
        "# Show the binary mask\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(mask, cmap='gray')\n",
        "plt.title(\"Binary Mask – Filament 0\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynsYTeZpfd_Q"
      },
      "source": [
        "# 📍 Phase 2: AOI Definition & Filtering\n",
        "\n",
        "In this phase, we define and apply spatial filters to extract filament annotations within key plastic accumulation zones in the Mediterranean Sea. These Areas of Interest (AOIs) include:\n",
        "\n",
        "- **Po River Plume** (Northern Adriatic)\n",
        "- **Northern Corsica**\n",
        "- **Gulf of Genova**\n",
        "\n",
        "### ✅ Objectives:\n",
        "- Define bounding boxes (~50x50 km) for each AOI.\n",
        "- Visualize the AOIs on a map using Cartopy.\n",
        "- Filter filament centroids to retain only those within each AOI.\n",
        "- Extract pixel-level annotations and bounding boxes for selected filaments.\n",
        "\n",
        "### ✅ Outcome:\n",
        "The dataset has been filtered successfully:\n",
        "- 1512 filaments in Po River Plume\n",
        "- 39 in Northern Corsica\n",
        "- 31 in Gulf of Genova\n",
        "\n",
        "The pixel coordinates and bounding boxes for each AOI are now ready for mask generation or patch extraction in the next phase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivLgxEe_e1qm"
      },
      "outputs": [],
      "source": [
        "# Define bounding boxes as [min_lon, min_lat, max_lon, max_lat]\n",
        "\n",
        "AOI_BBOXES = {\n",
        "    'Po_River_Plume':      [12.5, 44.8, 13.2, 45.5],   # Near the Po River delta (Northern Adriatic)\n",
        "    'Northern_Corsica':    [8.5, 42.9, 9.2, 43.6],     # North of Corsica island\n",
        "    'Gulf_of_Genova':      [8.5, 43.7, 9.2, 44.4],     # Ligurian Sea, near major coastal cities\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp1qnpBTfpx6"
      },
      "outputs": [],
      "source": [
        "!pip install cartopy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNfOLxTKfwlU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "\n",
        "# Bounding boxes from previous step\n",
        "AOI_BBOXES = {\n",
        "    'Po_River_Plume':      [12.5, 44.8, 13.2, 45.5],\n",
        "    'Northern_Corsica':    [8.5, 42.9, 9.2, 43.6],\n",
        "    'Gulf_of_Genova':      [8.5, 43.7, 9.2, 44.4],\n",
        "}\n",
        "\n",
        "# Plot setup\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = plt.axes(projection=ccrs.PlateCarree())\n",
        "ax.set_extent([-7, 37, 30, 46], crs=ccrs.PlateCarree())  # Mediterranean bounds\n",
        "ax.add_feature(cfeature.LAND)\n",
        "ax.add_feature(cfeature.OCEAN)\n",
        "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
        "ax.add_feature(cfeature.COASTLINE)\n",
        "\n",
        "# Plot AOIs\n",
        "for name, (min_lon, min_lat, max_lon, max_lat) in AOI_BBOXES.items():\n",
        "    ax.plot([min_lon, max_lon, max_lon, min_lon, min_lon],\n",
        "            [min_lat, min_lat, max_lat, max_lat, min_lat],\n",
        "            transform=ccrs.PlateCarree(), label=name)\n",
        "\n",
        "# Final touches\n",
        "plt.title(\"Defined AOIs for Plastic Detection (50x50 km approx.)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glekGDUgf2JC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load all centroids\n",
        "lat = ds.variables['lat_centroid'][:]\n",
        "lon = ds.variables['lon_centroid'][:]\n",
        "\n",
        "# Initialize results\n",
        "filtered_filaments = {\n",
        "    'Po_River_Plume': [],\n",
        "    'Northern_Corsica': [],\n",
        "    'Gulf_of_Genova': []\n",
        "}\n",
        "\n",
        "# Check which centroids fall in each AOI\n",
        "for i in range(len(lat)):\n",
        "    lat_i, lon_i = lat[i], lon[i]\n",
        "\n",
        "    for aoi, (min_lon, min_lat, max_lon, max_lat) in AOI_BBOXES.items():\n",
        "        if (min_lon <= lon_i <= max_lon) and (min_lat <= lat_i <= max_lat):\n",
        "            filtered_filaments[aoi].append(i)\n",
        "\n",
        "# Print summary\n",
        "for aoi in filtered_filaments:\n",
        "    print(f\"{aoi}: {len(filtered_filaments[aoi])} filaments found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_1ygi6AgLUs"
      },
      "outputs": [],
      "source": [
        "# Select AOI\n",
        "aoi_name = 'Po_River_Plume'\n",
        "filament_indices = filtered_filaments[aoi_name]\n",
        "\n",
        "# Load all relevant variables\n",
        "pixel_x_all = ds.variables['pixel_x']\n",
        "pixel_y_all = ds.variables['pixel_y']\n",
        "limits_all = ds.variables['limits']\n",
        "n_pixels_all = ds.variables['n_pixels_fil']\n",
        "\n",
        "# Store data for each filament\n",
        "aoi_filaments_data = []\n",
        "\n",
        "for idx in filament_indices:\n",
        "    n_pixels = n_pixels_all[idx]\n",
        "\n",
        "    # Get valid pixel coordinates\n",
        "    px = pixel_x_all[idx][:n_pixels]\n",
        "    py = pixel_y_all[idx][:n_pixels]\n",
        "\n",
        "    # Get bounding box: [x_min, y_min, x_max, y_max]\n",
        "    bbox = limits_all[idx]\n",
        "\n",
        "    aoi_filaments_data.append({\n",
        "        'index': idx,\n",
        "        'n_pixels': n_pixels,\n",
        "        'pixels_x': px,\n",
        "        'pixels_y': py,\n",
        "        'bbox': bbox\n",
        "    })\n",
        "\n",
        "# Print summary of first 3 filaments\n",
        "for i, f in enumerate(aoi_filaments_data[:3]):\n",
        "    print(f\"\\nFilament #{f['index']}\")\n",
        "    print(f\"Number of pixels: {f['n_pixels']}\")\n",
        "    print(f\"Bounding box: {f['bbox']}\")\n",
        "    print(f\"First 5 pixel coords: {list(zip(f['pixels_x'][:5], f['pixels_y'][:5]))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(aoi_filaments_data)"
      ],
      "metadata": {
        "id": "pyRqlvDcX7dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txgEiqschMQd"
      },
      "source": [
        "# 🧪 Phase 3: Segmentation Mask Generation (Test Batch)\n",
        "\n",
        "This phase focuses on creating binary segmentation masks from pixel-level annotations for plastic filaments.\n",
        "\n",
        "### ✅ Objectives:\n",
        "- Use pixel (x, y) coordinates and centroids from the Litter Windrows Catalog.\n",
        "- Generate binary masks of size 256×256 pixels centered around each filament.\n",
        "- Each mask uses:\n",
        "  - `1` to represent filament/presence of plastic.\n",
        "  - `0` to represent background (no plastic).\n",
        "- Validate the correctness of the masks through visualization.\n",
        "\n",
        "### 📦 Output:\n",
        "- A set of sample binary masks saved as `.png` images.\n",
        "- Each mask aligns with future Sentinel-2 patches for training segmentation models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEVhRAy9go8m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Create output folder\n",
        "output_folder = \"/content/masks_test_po_river\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Select first 5 filaments from Po River Plume\n",
        "sample_filaments = aoi_filaments_data[:5]\n",
        "\n",
        "# Define mask size and padding\n",
        "mask_size = 256  # 256x256 pixels\n",
        "half_size = mask_size // 2\n",
        "\n",
        "# Loop through each filament\n",
        "for i, filament in enumerate(sample_filaments):\n",
        "    # Get pixel coordinates for this filament\n",
        "    px = filament['pixels_x']\n",
        "    py = filament['pixels_y']\n",
        "\n",
        "    # Get centroid to center the patch\n",
        "    cx = ds.variables['x_centroid'][filament['index']]\n",
        "    cy = ds.variables['y_centroid'][filament['index']]\n",
        "\n",
        "    # Create an empty binary mask\n",
        "    mask = np.zeros((mask_size, mask_size), dtype=np.uint8)\n",
        "\n",
        "    # Map each filament pixel to the mask relative to centroid\n",
        "    for x, y in zip(px, py):\n",
        "        rel_x = x - cx + half_size\n",
        "        rel_y = y - cy + half_size\n",
        "\n",
        "        # Ensure coordinates are inside the mask\n",
        "        if 0 <= rel_x < mask_size and 0 <= rel_y < mask_size:\n",
        "            mask[rel_y, rel_x] = 1  # 1 = plastic pixel\n",
        "\n",
        "    # Save the binary mask as a PNG file\n",
        "    filename = f\"{output_folder}/mask_{filament['index']}.png\"\n",
        "    Image.fromarray(mask * 255).save(filename)\n",
        "\n",
        "    # Optional: Show the mask\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.title(f\"Mask for Filament #{filament['index']}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewMPT5VKi3fE"
      },
      "source": [
        "# 🌍 Phase 4: RGB Patch Retrieval via Google Earth Engine (GEE)\n",
        "\n",
        "This phase integrates Earth Engine into the pipeline to retrieve Sentinel-2 image patches aligned with plastic filament annotations.\n",
        "\n",
        "### ✅ Objectives:\n",
        "- Use centroid latitude, longitude, and acquisition date to locate the matching satellite imagery.\n",
        "- Query Sentinel-2 Surface Reflectance (`S2_SR`) as the primary source.\n",
        "- Automatically fall back to TOA (`S2`) when SR imagery is unavailable.\n",
        "- Clip 256×256 pixel RGB patches (B4, B3, B2) centered on each filament.\n",
        "\n",
        "### ✅ Tools:\n",
        "- Earth Engine Python API via `ee` and `geemap`\n",
        "- Sentinel-2 collections: `COPERNICUS/S2_SR` and `COPERNICUS/S2`\n",
        "\n",
        "### ✅ Outcome:\n",
        "An RGB image patch is now correctly retrieved and visualized for each filament.\n",
        "This confirms readiness for batch patch extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMLT3hNWi5gL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "os.makedirs(\"patch_metadata\", exist_ok=True)\n",
        "\n",
        "lat_all = ds.variables['lat_centroid']\n",
        "lon_all = ds.variables['lon_centroid']\n",
        "time_all = ds.variables['dec_time']\n",
        "s2_product_all = ds.variables['s2_product']\n",
        "\n",
        "rows = []\n",
        "for filament in sample_filaments:\n",
        "    idx = filament['index']\n",
        "\n",
        "    lat = float(lat_all[idx])\n",
        "    lon = float(lon_all[idx])\n",
        "    dec_year = float(time_all[idx])\n",
        "\n",
        "    year = int(dec_year)\n",
        "    remainder = dec_year - year\n",
        "    days = int(remainder * 365.25)\n",
        "    date_str = datetime(year, 1, 1) + pd.Timedelta(days=days)\n",
        "    date = date_str.strftime('%Y-%m-%d')\n",
        "\n",
        "    # ✅ FIXED: Decode the product name correctly\n",
        "    s2_product = s2_product_all[idx].tobytes().decode('utf-8').strip()\n",
        "\n",
        "    rows.append({\n",
        "        'filament_id': idx,\n",
        "        'aoi': 'Po_River_Plume',\n",
        "        'latitude': lat,\n",
        "        'longitude': lon,\n",
        "        'acquisition_date': date,\n",
        "        's2_product': s2_product\n",
        "    })\n",
        "\n",
        "# Create DataFrame and save\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(\"patch_metadata/po_river_test_patches.csv\", index=False)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfryYpdukomp"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LUhBSSAjoDM"
      },
      "outputs": [],
      "source": [
        "!pip install earthengine-api geemap --quiet\n",
        "\n",
        "import ee\n",
        "import geemap\n",
        "\n",
        "# Authenticate once (opens Google login)\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize the Earth Engine session\n",
        "ee.Initialize(project='ee-mujtabanaqvi29')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSUJryS3k7j6"
      },
      "outputs": [],
      "source": [
        "row = df.iloc[0]\n",
        "\n",
        "# Geometry setup\n",
        "point = ee.Geometry.Point([row['longitude'], row['latitude']])\n",
        "patch_size = 1280  # 1280 meters = 128 pixels on either side (256x256 patch)\n",
        "region = point.buffer(patch_size).bounds()\n",
        "\n",
        "# Date setup\n",
        "date_start = row['acquisition_date']\n",
        "date_end = pd.to_datetime(date_start) + pd.Timedelta(days=10)\n",
        "date_end_str = date_end.strftime('%Y-%m-%d')\n",
        "\n",
        "# Try S2_SR first\n",
        "print(\"Trying S2_SR...\")\n",
        "image_sr = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "            .filterBounds(point)\n",
        "            .filterDate(date_start, date_end_str)\n",
        "            .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
        "            .first())\n",
        "\n",
        "if image_sr and image_sr.getInfo():  # Check if image_sr is not null\n",
        "    print(\"✅ Using Surface Reflectance image.\")\n",
        "    image = image_sr.select(['B4', 'B3', 'B2'])\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ No S2_SR image found. Trying S2 TOA...\")\n",
        "    image_toa = (ee.ImageCollection('COPERNICUS/S2')\n",
        "                 .filterBounds(point)\n",
        "                 .filterDate(date_start, date_end_str)\n",
        "                 .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
        "                 .first())\n",
        "\n",
        "    if image_toa and image_toa.getInfo():\n",
        "        print(\"✅ Using TOA image.\")\n",
        "        image = image_toa.select(['B4', 'B3', 'B2'])\n",
        "    else:\n",
        "        raise ValueError(\"❌ No suitable image found in either S2_SR or S2 TOA collections.\")\n",
        "\n",
        "# Clip to patch area\n",
        "rgb_patch = image.clip(region)\n",
        "\n",
        "# Display in Colab\n",
        "Map = geemap.Map()\n",
        "Map.centerObject(region, 12)\n",
        "Map.addLayer(rgb_patch, {'min': 0, 'max': 3000}, 'RGB Patch')\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9jDnbDtoAp9"
      },
      "source": [
        "# 📦 Phase 5: Batch Export of Image Patches to Google Drive\n",
        "\n",
        "This phase involves exporting Sentinel-2 image patches from Earth Engine for each plastic filament detected in the Litter Windrows Catalog.\n",
        "\n",
        "### ✅ Objectives:\n",
        "- Export 256×256 pixel RGB patches centered on filament centroids.\n",
        "- Use GEE's batch export capabilities to send patches to Google Drive.\n",
        "- Start with a small batch (e.g., 5 test patches) to validate the workflow.\n",
        "- Prepare for scaling to all 1582 filtered filaments across the 3 AOIs.\n",
        "\n",
        "### ✅ Export Settings:\n",
        "- **Resolution**: 10 meters per pixel\n",
        "- **Size**: 2.56×2.56 km (256×256 pixels)\n",
        "- **Bands**: B4 (Red), B3 (Green), B2 (Blue)\n",
        "- **Format**: `.tif` files (GeoTIFF)\n",
        "- **Output**: Saved in a folder on Google Drive for later download\n",
        "\n",
        "➡️ Once verified, this batch export will be scaled to all AOIs and used alongside binary masks for segmentation model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_-tn3zBlIkV"
      },
      "outputs": [],
      "source": [
        "# Export folder name in Google Drive\n",
        "drive_folder = 'plastic_patches_test'\n",
        "\n",
        "# Patch size\n",
        "patch_size_m = 1280  # 1280 meters = 256×256 at 10m resolution\n",
        "\n",
        "# Loop through first 5 rows\n",
        "for i in range(5):\n",
        "    row = df.iloc[i]\n",
        "    lat, lon = row['latitude'], row['longitude']\n",
        "    date_start = row['acquisition_date']\n",
        "    date_end = pd.to_datetime(date_start) + pd.Timedelta(days=10)\n",
        "\n",
        "    point = ee.Geometry.Point([lon, lat])\n",
        "    region = point.buffer(patch_size_m).bounds()\n",
        "    image = None\n",
        "\n",
        "    # Try Surface Reflectance\n",
        "    s2_sr = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "             .filterBounds(point)\n",
        "             .filterDate(date_start, date_end.strftime('%Y-%m-%d'))\n",
        "             .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
        "             .first())\n",
        "\n",
        "    if s2_sr and s2_sr.getInfo():\n",
        "        image = s2_sr.select(['B4', 'B3', 'B2'])\n",
        "        print(f\"✅ Using S2_SR for filament {row['filament_id']}\")\n",
        "    else:\n",
        "        # Fallback to TOA\n",
        "        s2_toa = (ee.ImageCollection('COPERNICUS/S2')\n",
        "                  .filterBounds(point)\n",
        "                  .filterDate(date_start, date_end.strftime('%Y-%m-%d'))\n",
        "                  .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
        "                  .first())\n",
        "        if s2_toa and s2_toa.getInfo():\n",
        "            image = s2_toa.select(['B4', 'B3', 'B2'])\n",
        "            print(f\"⚠️ Fallback to S2 TOA for filament {row['filament_id']}\")\n",
        "        else:\n",
        "            print(f\"❌ No image found for filament {row['filament_id']}\")\n",
        "            continue  # skip if no image found\n",
        "\n",
        "    # Set export task\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=image.clip(region),\n",
        "        description=f'filament_patch_{int(row[\"filament_id\"])}',\n",
        "        folder=drive_folder,\n",
        "        fileNamePrefix=f'filament_patch_{int(row[\"filament_id\"])}',\n",
        "        region=region.getInfo()['coordinates'],\n",
        "        scale=10,\n",
        "        maxPixels=1e9,\n",
        "        fileFormat='GeoTIFF'\n",
        "    )\n",
        "\n",
        "    task.start()\n",
        "    print(f\"🚀 Export task started for filament {row['filament_id']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnjRQmI4sUwq"
      },
      "source": [
        "# 🧪 Phase 6: Dataset Preparation for Model Training\n",
        "\n",
        "This phase focuses on preparing the image–mask pairs for training a segmentation model (e.g., UNet with fast.ai or PyTorch).\n",
        "\n",
        "### ✅ Objectives:\n",
        "- Download `.tif` image patches from Google Drive into Colab.\n",
        "- Match them with corresponding `.png` binary masks.\n",
        "- Organize everything into:\n",
        "  - `dataset/images/` – Input satellite image patches\n",
        "  - `dataset/masks/` – Ground truth segmentation masks\n",
        "- Visualize image–mask pairs to ensure alignment.\n",
        "- Prepare data for direct use in model training (next phase).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itoHI8MmsGUb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define destination folders\n",
        "local_image_dir = 'dataset/images'\n",
        "local_mask_dir = 'dataset/masks'\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(local_image_dir, exist_ok=True)\n",
        "os.makedirs(local_mask_dir, exist_ok=True)\n",
        "\n",
        "# Copy image patches (.tif) from Drive folder\n",
        "patch_source = '/content/drive/MyDrive/plastic_patches_test'\n",
        "for file in os.listdir(patch_source):\n",
        "    if file.endswith('.tif'):\n",
        "        shutil.copyfile(f\"{patch_source}/{file}\", f\"{local_image_dir}/{file}\")\n",
        "\n",
        "# Copy corresponding masks (.png)\n",
        "mask_source = '/content/masks_test_po_river'\n",
        "for file in os.listdir(mask_source):\n",
        "    if file.endswith('.png'):\n",
        "        shutil.copyfile(f\"{mask_source}/{file}\", f\"{local_mask_dir}/{file}\")\n",
        "\n",
        "print(\"✅ Image patches and masks organized in 'dataset/images' and 'dataset/masks'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZsn6FPStTVm"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJy6tpXAsvcK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import rasterio\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "# Load one image–mask pair by filament ID\n",
        "filament_id = 3325  # Change to test others , other filament_ids = [3298, 3299, 3300, 3311, 3325]\n",
        "\n",
        "# File paths\n",
        "img_path = f\"dataset/images/filament_patch_{filament_id}.tif\"\n",
        "mask_path = f\"dataset/masks/mask_{filament_id}.png\"\n",
        "\n",
        "# Load image (rasterio handles GeoTIFF)\n",
        "with rasterio.open(img_path) as src:\n",
        "    rgb = src.read([1, 2, 3])  # B4, B3, B2\n",
        "    rgb = np.transpose(rgb, (1, 2, 0))  # Convert to HWC for display\n",
        "\n",
        "# Normalize for visualization\n",
        "rgb_vis = (rgb / np.max(rgb)) if np.max(rgb) > 1 else rgb\n",
        "\n",
        "# Load mask\n",
        "mask = Image.open(mask_path)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(rgb_vis)\n",
        "plt.title(\"RGB Image Patch\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(mask, cmap='gray')\n",
        "plt.title(\"Binary Mask\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snfDPG1evlQH"
      },
      "source": [
        "# ☁️ Phase 7: Cloud Filtering Strategy & Export Planning\n",
        "\n",
        "This section outlines the strategy for filtering and exporting high-quality Sentinel-2 image patches for plastic detection model training.\n",
        "\n",
        "### ✅ Strategy Overview:\n",
        "1. **Cloud Filtering Assessment**\n",
        "   - Check how many of the filtered filaments are usable after applying:\n",
        "     - `< 20%` cloud threshold\n",
        "     - `< 10%` cloud threshold\n",
        "   - Use Sentinel-2 Surface Reflectance (`S2_SR`) collection for accurate metadata.\n",
        "\n",
        "2. **Visual Inspection**\n",
        "   - Manually check more sample image–mask pairs after filtering.\n",
        "   - Assess quality: glint, haze, clipping, or heavy cloud coverage.\n",
        "\n",
        "3. **Download Plan**\n",
        "   - Export image patches in two formats for all accepted samples:\n",
        "     - **RGB only** (B4, B3, B2) – for lightweight models\n",
        "     - **All bands** (13 bands) – for advanced spectral training\n",
        "   - Use Earth Engine export functionality and organize output into appropriate training folders.\n",
        "\n",
        "### 📁 Output:\n",
        "- Number of usable filaments per threshold\n",
        "- Visual confirmation of patch quality\n",
        "- Export scripts ready for full dataset extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYeTqIQYx0lg"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "rows = []\n",
        "\n",
        "for aoi, indices in filtered_filaments.items():\n",
        "    for idx in indices:\n",
        "        lat = float(ds.variables['lat_centroid'][idx])\n",
        "        lon = float(ds.variables['lon_centroid'][idx])\n",
        "        dec_year = float(ds.variables['dec_time'][idx])\n",
        "\n",
        "        year = int(dec_year)\n",
        "        remainder = dec_year - year\n",
        "        days = int(remainder * 365.25)\n",
        "        date_str = datetime(year, 1, 1) + pd.Timedelta(days=days)\n",
        "        date = date_str.strftime('%Y-%m-%d')\n",
        "\n",
        "        s2_product = ds.variables['s2_product'][idx].tobytes().decode('utf-8').strip()\n",
        "\n",
        "        rows.append({\n",
        "            'filament_id': idx,\n",
        "            'aoi': aoi,\n",
        "            'latitude': lat,\n",
        "            'longitude': lon,\n",
        "            'acquisition_date': date,\n",
        "            's2_product': s2_product\n",
        "        })\n",
        "\n",
        "# Save as CSV\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(\"all_filtered_filaments.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Saved metadata for {len(df)} filaments to all_filtered_filaments.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hM3ZvdYx5VR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"all_filtered_filaments.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1pG08Ncvk2u"
      },
      "outputs": [],
      "source": [
        "\"\"\" Dont run this cell, the next code cell accomodate this\n",
        "# Set thresholds to test\n",
        "thresholds = [20, 10]\n",
        "results = {thresh: {\"accepted\": 0, \"rejected\": 0} for thresh in thresholds}\n",
        "\n",
        "for i in range(len(df)):\n",
        "    row = df.iloc[i]\n",
        "    lat, lon = row['latitude'], row['longitude']\n",
        "    date_start = row['acquisition_date']\n",
        "    date_end = pd.to_datetime(date_start) + pd.Timedelta(days=10)\n",
        "\n",
        "    point = ee.Geometry.Point([lon, lat])\n",
        "\n",
        "    try:\n",
        "        image = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "                 .filterBounds(point)\n",
        "                 .filterDate(date_start, date_end.strftime('%Y-%m-%d'))\n",
        "                 .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
        "                 .first())\n",
        "\n",
        "        cloud_pct = image.get('CLOUDY_PIXEL_PERCENTAGE').getInfo()\n",
        "\n",
        "        for thresh in thresholds:\n",
        "            if cloud_pct < thresh:\n",
        "                results[thresh]['accepted'] += 1\n",
        "            else:\n",
        "                results[thresh]['rejected'] += 1\n",
        "\n",
        "    except:\n",
        "        for thresh in thresholds:\n",
        "            results[thresh]['rejected'] += 1\n",
        "\n",
        "# Print results\n",
        "for thresh in thresholds:\n",
        "    total = results[thresh]['accepted'] + results[thresh]['rejected']\n",
        "    print(f\"☁️ Threshold < {thresh}%\")\n",
        "    print(f\"   ✅ Accepted: {results[thresh]['accepted']}\")\n",
        "    print(f\"   ❌ Rejected: {results[thresh]['rejected']}\")\n",
        "    print(f\"   📊 Total Checked: {total}\\n\")\n",
        "    \"\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBDNgyJy2h26"
      },
      "source": [
        "# ☁️ Phase 8: Cloud-Filtered Export of Image Patches (10% Threshold)\n",
        "\n",
        "This section initiates the full export of Sentinel-2 image patches, using only high-quality samples where the total scene cloud coverage is less than 10%. This improves the reliability and visibility of plastic litter filaments in the dataset.\n",
        "\n",
        "### ✅ Objectives:\n",
        "- Filter out scenes with >10% cloud coverage using `CLOUDY_PIXEL_PERCENTAGE`\n",
        "- Limit training and testing images to only the 1234 cleanest filament detections\n",
        "- Export two types of image patches for each filament:\n",
        "  - **RGB only** (B4, B3, B2)\n",
        "  - **Multispectral MARIDA-style (11 bands)** for full spectral model training\n",
        "- All patches are 256×256 pixels (2.56×2.56 km at 10m resolution), centered on each filament\n",
        "- Binary masks generated earlier will be reused to train segmentation models\n",
        "\n",
        "### 📁 Output Folder Structure:\n",
        "- `plastic_patches_rgb/` – 1234 RGB `.tif` patches (B4, B3, B2)\n",
        "- `plastic_patches_allbands/` – 1234 11-band `.tif` patches (MARIDA-style)\n",
        "- `dataset/masks/` – Binary `.png` masks (already generated)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LUG1tQY8nxm"
      },
      "source": [
        "The next snippet takes around 30-45 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvFKZBzJwntN"
      },
      "outputs": [],
      "source": [
        "# Load full metadata again\n",
        "df = pd.read_csv(\"all_filtered_filaments.csv\")\n",
        "\n",
        "accepted_10pct_ids = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    row = df.iloc[i]\n",
        "    lat, lon = row['latitude'], row['longitude']\n",
        "    date_start = row['acquisition_date']\n",
        "    date_end = pd.to_datetime(date_start) + pd.Timedelta(days=10)\n",
        "\n",
        "    point = ee.Geometry.Point([lon, lat])\n",
        "\n",
        "    try:\n",
        "        image = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "                 .filterBounds(point)\n",
        "                 .filterDate(date_start, date_end.strftime('%Y-%m-%d'))\n",
        "                 .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
        "                 .first())\n",
        "\n",
        "        cloud_pct = image.get('CLOUDY_PIXEL_PERCENTAGE').getInfo()\n",
        "\n",
        "        if cloud_pct < 10:\n",
        "            accepted_10pct_ids.append(row['filament_id'])\n",
        "\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Filter the original DataFrame and save\n",
        "df_filtered = df[df['filament_id'].isin(accepted_10pct_ids)].reset_index(drop=True)\n",
        "df_filtered.to_csv(\"filtered_cloud10_filaments.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Saved {len(df_filtered)} filaments with <10% cloud to 'filtered_cloud10_filaments.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO0AFe6C8f79"
      },
      "source": [
        "The Following snippet donwloads the first 300 images only. (not checked so far), will take alot of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqhfBpxa8_NG"
      },
      "outputs": [],
      "source": [
        "# Load only first 300 rows\n",
        "df = pd.read_csv(\"filtered_cloud10_filaments.csv\").head(300)\n",
        "\n",
        "patch_size_m = 1280\n",
        "rgb_bands = ['B4', 'B3', 'B2']\n",
        "marida_bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    lat, lon = row['latitude'], row['longitude']\n",
        "    date_start = row['acquisition_date']\n",
        "    date_end = pd.to_datetime(date_start) + pd.Timedelta(days=10)\n",
        "    point = ee.Geometry.Point([lon, lat])\n",
        "    region = point.buffer(patch_size_m).bounds()\n",
        "    fid = int(row['filament_id'])\n",
        "\n",
        "    image = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "             .filterBounds(point)\n",
        "             .filterDate(date_start, date_end.strftime('%Y-%m-%d'))\n",
        "             .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
        "             .first())\n",
        "\n",
        "    try:\n",
        "        image.getInfo()\n",
        "    except:\n",
        "        print(f\"❌ Skipping {fid}, no image found\")\n",
        "        continue\n",
        "\n",
        "    # Export RGB\n",
        "    ee.batch.Export.image.toDrive(\n",
        "        image=image.select(rgb_bands).clip(region),\n",
        "        description=f'RGB_patch_{fid}',\n",
        "        folder='plastic_patches_rgb',\n",
        "        fileNamePrefix=f'patch_rgb_{fid}',\n",
        "        region=region.getInfo()['coordinates'],\n",
        "        scale=10,\n",
        "        maxPixels=1e9\n",
        "    ).start()\n",
        "\n",
        "    # Export MARIDA bands (optional, disable if not needed)\n",
        "    ee.batch.Export.image.toDrive(\n",
        "        image=image.select(marida_bands).clip(region),\n",
        "        description=f'AllBands_patch_{fid}',\n",
        "        folder='plastic_patches_allbands',\n",
        "        fileNamePrefix=f'patch_allbands_{fid}',\n",
        "        region=region.getInfo()['coordinates'],\n",
        "        scale=10,\n",
        "        maxPixels=1e9\n",
        "    ).start()\n",
        "\n",
        "    print(f\"🚀 Export task started for filament {fid} [{i+1}/300]\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}